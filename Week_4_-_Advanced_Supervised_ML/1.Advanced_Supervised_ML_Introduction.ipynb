{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902f62a8",
   "metadata": {},
   "source": [
    "# Advanced Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ec67a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour (KNN)\n",
    "\n",
    "K-Nearest Neighbour is a simple yet powerful **instance-based learning algorithm** used for both classification and regression tasks. Unlike parametric models that learn a function during training, KNN is a **lazy learner** that stores all training data and makes predictions by finding the K closest examples to a new data point.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand how KNN makes predictions using distance metrics, the impact of the K value, and the differences between KNN classification and regression.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d963353",
   "metadata": {},
   "source": [
    "#### How KNN Works: The Algorithm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[New Data Point] --> B[Calculate Distance to<br/>All Training Points]\n",
    "    B --> C[Sort Distances<br/>Ascending Order]\n",
    "    C --> D[Select K Nearest<br/>Neighbours]\n",
    "    D --> E{Task Type?}\n",
    "    E -->|Classification| F[Vote: Most Common<br/>Class Among K]\n",
    "    E -->|Regression| G[Average: Mean Value<br/>Among K]\n",
    "    F --> H[Predicted Class]\n",
    "    G --> I[Predicted Value]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#ffeaa7,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style G fill:#d4edda,color:#333\n",
    "    style H fill:#f8d7da,color:#333\n",
    "    style I fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Store Training Data**: KNN stores all training examples (lazy learning)\n",
    "2. **Calculate Distance**: When a new point arrives, calculate distance to all training points\n",
    "3. **Find K Nearest**: Select the K closest training examples\n",
    "4. **Make Prediction**:\n",
    "   - **Classification**: Majority vote among K neighbours\n",
    "   - **Regression**: Average (mean) of K neighbours' values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a223262",
   "metadata": {},
   "source": [
    "#### Choosing the K Value\n",
    "\n",
    "The value of K significantly impacts model performance:\n",
    "\n",
    "| K Value | Behaviour | Advantages | Disadvantages |\n",
    "| --- | --- | --- | --- |\n",
    "| K = 1 | Uses only nearest neighbour | Simple, captures fine patterns | Sensitive to noise, overfitting |\n",
    "| K = 3-5 | Small neighbourhood | Good balance for many problems | May still be affected by noise |\n",
    "| K = 10-20 | Medium neighbourhood | More stable, less noise sensitive | May miss local patterns |\n",
    "| K = Large | Uses many neighbours | Smooth decision boundaries | Underfitting, ignores local structure |\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Small K<br/>e.g., K=1] --> B[Complex Boundaries<br/>High Variance<br/>Overfitting Risk]\n",
    "    C[Medium K<br/>e.g., K=5-10] --> D[Balanced<br/>Good Generalization]\n",
    "    E[Large K<br/>e.g., K=50] --> F[Smooth Boundaries<br/>High Bias<br/>Underfitting Risk]\n",
    "    \n",
    "    style A fill:#f8d7da,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style E fill:#fff3cd,color:#333\n",
    "```\n",
    "\n",
    "> [!Note]\n",
    ">\n",
    "> **Best Practice**: Always use an **odd number** for K in binary classification to avoid ties. Use cross-validation to find the optimal K value for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b676a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from utils_common import generate_data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random data set\n",
    "m = generate_data(0, 50, 0, 50, 150, 0.4)\n",
    "n = generate_data(0, 50, 0, 50, 150, 0.45)\n",
    "o = generate_data(30, 50, 30, 50, 10, 0.1)\n",
    "\n",
    "cols = [random.randint(0, 1) for _ in range(10)]\n",
    "radii = [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb234f",
   "metadata": {},
   "source": [
    "> [!Tip]\n",
    "> Toggle `zoom = False` below to zoom in and out of KNN to see the algorithm at the local level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea05144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Regression\n",
    "# Mean of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = False\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='black')\n",
    "    plt.scatter(n[0], n[1], color='black')\n",
    "else:\n",
    "    plt.scatter(o[0], o[1], color='black')\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"K-Nearest Neighbour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864b7b2",
   "metadata": {},
   "source": [
    "> [!Tip]\n",
    "> Toggle `zoom = False` below to zoom in and out of KNN to see the algorithm at the local level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Classification\\\n",
    "# Mode Class of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = False\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='gold', label='Class 0')\n",
    "    plt.scatter(n[0], n[1], color='indigo', label='Class 1')\n",
    "else:\n",
    "    plt.scatter([], [], c='gold', label='Class 0')\n",
    "    plt.scatter([], [], c='indigo', label='Class 1')\n",
    "    plt.scatter(o[0], o[1], c=cols)\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for radius in radii:\n",
    "        circle = Circle((40, 40), radius, color='blue', fill=False, linestyle='--')\n",
    "        ax.add_patch(circle)\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"K-Nearest Neighbour Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bafd0d",
   "metadata": {},
   "source": [
    "#### Measuring Distance\n",
    "\n",
    "KNN uses either Euclidean distance (straight-line) and Manhattan distance (grid-like, right-angle path) between two points. Although Euclidean distance is the most commenly used measure in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Euclidean & Manhattan Distance\n",
    "A, B = np.array([2, 3]), np.array([8, 7])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(*A, color='blue', s=100, label='A')\n",
    "ax.scatter(*B, color='red', s=100, label='B')\n",
    "# Euclidean (straight line)\n",
    "ax.plot([A[0], B[0]], [A[1], B[1]], color='green', lw=2, label='Euclidean')\n",
    "# Manhattan (right-angle path)\n",
    "ax.plot([A[0], B[0]], [A[1], A[1]], color='orange', ls='--', lw=2)\n",
    "ax.plot([B[0], B[0]], [A[1], B[1]], color='orange', ls='--', lw=2, label='Manhattan')\n",
    "ax.annotate('A', A + [0.2, -0.2], fontsize=12, color='blue')\n",
    "ax.annotate('B', B + [0.2, 0.2], fontsize=12, color='red')\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.set(xlim=(0, 10), ylim=(0, 10), aspect='equal', xlabel='X', ylabel='Y', title='Euclidean vs Manhattan Distance')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b81c1",
   "metadata": {},
   "source": [
    "### Neural Network Course Specifications\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"images\\NN_Course-Specs.png\" alt=\"Course Specs Neural Network image\" width=\"500\" />\n",
    "    <figcaption><p><em>Source: Page 29 of the Software Engineering Course Specifications</em></p>\n",
    "    </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Neural networks were designed to mimic the processing inside the human brain. They consist of a series of interconnected nodes (artificial neurones). Each neurone can accept a binary input signal and potentially output another signal to connected nodes.\n",
    "\n",
    "#### Training cycle\n",
    "\n",
    "Internal weightings and threshold values for each node are determined in the initial training cycle for each neural network. The system is exposed to a series of inputs with known responses. Linear regression with backward chaining is used to iteratively determine the set of unique values required for output. Regular exposure to the training cycle results in improved accuracy and pattern matching.\n",
    "\n",
    "#### Execution cycle\n",
    "\n",
    "In the diagram, signal strength between nodes with the strongest weightings are thicker representing a higher priority in determining the final output. The execution cycle follows the training cycle and utilises the internal values developed during the training cycle to determine the output.\n",
    "\n",
    "Page 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776f489",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision Trees are a powerful and interpretable supervised learning algorithm that creates a tree-like model of decisions. They work by recursively splitting the data based on feature values, creating a hierarchical structure that mimics human decision-making processes.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand how decision trees split data, the concepts of entropy and information gain, and how to interpret tree structures.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8bf3e",
   "metadata": {},
   "source": [
    "#### Decision Tree Structure\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Root Node<br/>Feature: Age<br/>Age ≤ 30?] -->|Yes| B[Internal Node<br/>Feature: Income<br/>Income ≤ 50K?]\n",
    "    A -->|No| C[Internal Node<br/>Feature: Credit Score<br/>Score ≤ 650?]\n",
    "    \n",
    "    B -->|Yes| D[Leaf Node<br/>Prediction: Deny]\n",
    "    B -->|No| E[Leaf Node<br/>Prediction: Approve]\n",
    "    \n",
    "    C -->|Yes| F[Leaf Node<br/>Prediction: Deny]\n",
    "    C -->|No| G[Leaf Node<br/>Prediction: Approve]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style F fill:#f8d7da,color:#333\n",
    "    style G fill:#d4edda,color:#333\n",
    "```\n",
    "\n",
    "**Tree Components:**\n",
    "\n",
    "- **Root Node**: Starting point, contains entire dataset\n",
    "- **Internal Nodes**: Decision points based on feature tests\n",
    "- **Branches**: Represent outcomes of tests (Yes/No, or feature ranges)\n",
    "- **Leaf Nodes**: Terminal nodes containing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a879b0",
   "metadata": {},
   "source": [
    "#### How Decision Trees Build: The Algorithm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start with<br/>Entire Dataset] --> B[Calculate Impurity<br/>Current Node]\n",
    "    B --> C[For Each Feature]\n",
    "    C --> D[Try All Possible<br/>Split Points]\n",
    "    D --> E[Calculate Information Gain<br/>or Gini Decrease]\n",
    "    E --> F[Select Best Split<br/>Highest Info Gain]\n",
    "    F --> G[Split Data into<br/>Left & Right Subsets]\n",
    "    G --> H{Stopping<br/>Criterion Met?}\n",
    "    H -->|No| I[Recursively Build<br/>Subtrees]\n",
    "    I --> B\n",
    "    H -->|Yes| J[Create Leaf Node<br/>with Prediction]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style H fill:#ffeaa7,color:#333\n",
    "    style J fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Root Node**: The algorithm starts with the entire dataset\n",
    "2. **Splitting**: At each node, select the feature and split value that best separates the data, minimizing impurity (entropy or Gini)\n",
    "3. **Recursion**: Repeat the process for each subset, creating new nodes and branches\n",
    "4. **Stopping Criteria**: Stop when:\n",
    "   - Maximum tree depth reached\n",
    "   - Minimum samples per leaf reached\n",
    "   - No further information gain possible\n",
    "   - All samples in node belong to same class\n",
    "5. **Leaf Nodes**: Terminal nodes contain the final predictions\n",
    "6. **Prediction**: Follow the path from root to leaf based on feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a84a3",
   "metadata": {},
   "source": [
    "#### Information Gain and Impurity Measures\n",
    "\n",
    "Decision trees use **impurity measures** to determine the best splits:\n",
    "\n",
    "| Measure | Formula | Use Case | Range |\n",
    "| --- | --- | --- | --- |\n",
    "| **Entropy** | $-\\sum p_i \\log_2(p_i)$ | ID3, C4.5 algorithms | 0 (pure) to 1 (impure) |\n",
    "| **Gini Impurity** | $1 - \\sum p_i^2$ | CART algorithm (scikit-learn default) | 0 (pure) to 0.5 (impure) |\n",
    "| **Variance** | $\\frac{1}{n}\\sum(y_i - \\bar{y})^2$ | Regression trees | Varies by data |\n",
    "\n",
    "**Information Gain** = Impurity(parent) - Weighted Average of Impurity(children)\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[High Entropy<br/>Mixed Classes<br/>Impure] --> B[Split on<br/>Best Feature]\n",
    "    B --> C[Lower Entropy<br/>More Homogeneous<br/>Purer]\n",
    "    C --> D[Repeat Until<br/>Pure Leaf Nodes]\n",
    "    \n",
    "    style A fill:#f8d7da,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "```\n",
    "\n",
    "> [!Note]\n",
    ">\n",
    "> The goal is to maximize **Information Gain** at each split, creating subsets that are as pure (homogeneous) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070251d0",
   "metadata": {},
   "source": [
    "#### Decision Trees: Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| Easy to understand and interpret (white-box model) | Prone to overfitting, especially with deep trees |\n",
    "| Requires little data preparation (no scaling needed) | Can be unstable (small data changes → different tree) |\n",
    "| Handles both numerical and categorical data | Biased toward features with more levels |\n",
    "| Can model non-linear relationships | Greedy algorithm (may not find global optimum) |\n",
    "| Works with missing values (some implementations) | Can create over-complex trees (poor generalization) |\n",
    "| Automatic feature selection through splits | Difficulty capturing XOR and other complex patterns |\n",
    "\n",
    "> [!Important]\n",
    ">\n",
    "> **Overfitting Prevention**: Use techniques like pruning, setting max_depth, min_samples_split, and min_samples_leaf to prevent overfitting. Consider using **Random Forests** (ensemble of decision trees) for better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68b488",
   "metadata": {},
   "source": [
    "#### Comparing KNN, Decision Trees, and Neural Networks\n",
    "\n",
    "| Aspect | K-Nearest Neighbour | Decision Trees | Neural Networks |\n",
    "| --- | --- | --- | --- |\n",
    "| **Learning Type** | Lazy (instance-based) | Eager (model-based) | Eager (model-based) |\n",
    "| **Training Time** | Fast (just stores data) | Moderate (builds tree) | Slow (iterative optimization) |\n",
    "| **Prediction Time** | Slow (calculates all distances) | Fast (traverses tree) | Fast (forward propagation) |\n",
    "| **Interpretability** | Low (black box) | High (visual tree) | Very Low (black box) |\n",
    "| **Feature Scaling** | Required | Not required | Required |\n",
    "| **Handling Non-linearity** | Good | Excellent | Excellent |\n",
    "| **Memory Usage** | High (stores all data) | Low (stores tree structure) | Moderate (stores weights) |\n",
    "| **Overfitting Risk** | Low K → High risk | Deep trees → High risk | Many neurons/layers → High risk |\n",
    "| **Data Requirements** | Small to medium datasets | Small to medium datasets | Large datasets preferred |\n",
    "| **Best Use Case** | Small datasets, Pattern recognition | Interpretability needed, Mixed data types | Complex patterns, Large datasets, Image/Text |\n",
    "\n",
    "> [!Note]\n",
    ">\n",
    "> Each algorithm has its strengths: **KNN** for simplicity and small datasets, **Decision Trees** for interpretability and mixed data types, and **Neural Networks** for complex patterns in large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
